
# ===================================================================
#               PARTIE 1 : Menu avec bouton d√©connexion
# ===================================================================

# 1.1 Cr√©er un template de base avec menu

# Modifions base.html pour ajouter une navbar :
nano templates/base.html

# 1.2 Mettre √† jour le template hello.html
nano templates/authentication/hello.html

# ===================================================================
#               PARTIE 2 : Page de Chat IA avec Ollama
# ===================================================================

# 2.1 Installer les d√©pendances

# bash
# Sur la VM Django
cd /opt/django_app
source venv/bin/activate

# Installer les biblioth√®ques n√©cessaires
pip install requests

# 2.2 Configuration settings.py
nano myproject/settings.py

# 2.3 Cr√©er le service Ollama
nano authentication/ollama_service.py

# 2.4 Cr√©er le mod√®le pour l'historique
nano authentication/models.py

# 2.5 Cr√©er les vues
nano authentication/views.py

# 2.6 URLs
nano authentication/urls.py

# 2.7 Template du chat IA
nano templates/authentication/ai_chat.html

# ===================================================================
#               PARTIE 3 : Configuration et d√©ploiement
# ===================================================================

# 3.1 Migrations

cd /opt/django_app
source venv/bin/activate

python manage.py makemigrations
python manage.py migrate

# 3.2 Mettre √† jour l'admin
nano authentication/admin.py

# 3.3 Configuration de l'IP de votre VM Ollama
nano myproject/settings.py
Modifiez cette ligne avec la bonne IP :
OLLAMA_API_URL = 'http://192.168.1.XXX:11434'  # Remplacez XXX par l'IP de votre VM Ollama
OLLAMA_DEFAULT_MODEL = 'llama2'  # Ou le mod√®le que vous avez install√©

# 3.4 Red√©marrer Gunicorn
sudo systemctl restart gunicorn

# ===================================================================
#      PARTIE 4 : Configuration d'Ollama (sur la VM IA voisine)
# ===================================================================

# Sur votre VM o√π Ollama est install√© :
# 4.1 Permettre les connexions externes

# √âditer le service Ollama
sudo nano /etc/systemd/system/ollama.service

# Ajoutez dans la section [Service] :
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Red√©marrez :
sudo systemctl daemon-reload
sudo systemctl restart ollama

# 4.2 V√©rifier les mod√®les install√©s
ollama list

# Si aucun mod√®le n'est install√© :

# Installer un mod√®le (ex: llama2)
ollama pull llama2

# Ou un mod√®le plus l√©ger
ollama pull mistral


# Test final

# Allez sur https://django.leumaire.fr
# Connectez-vous avec 2FA
# Cliquez sur üí¨ Chat IA dans le menu
# S√©lectionnez un mod√®le






















